# Projet d'Examen Nginx pour MLOps ğŸš€

Ce projet est une implÃ©mentation d'une architecture conteneurisÃ©e mettant en avant l'utilisation de Nginx pour le dÃ©ploiement, la sÃ©curisation, l'optimisation et le monitoring d'une API de Machine Learning.

## AperÃ§u du Projet

Ce projet dÃ©ploie et intÃ¨gre plusieurs outils fondamentaux pour la mise en production de modÃ¨les ML :

- **API ModÃ¨le ML (FastAPI)** : Une API simple pour la prÃ©diction de la classification d'Iris.
- **Docker** : UtilisÃ© pour conteneuriser l'API et Nginx, garantissant isolation et portabilitÃ©.
- **Nginx** : Sert de point d'entrÃ©e central, de proxy inverse, d'Ã©quilibreur de charge, et gÃ¨re les aspects de sÃ©curitÃ© avancÃ©s (HTTPS, authentification, limitation de dÃ©bit) ainsi que les stratÃ©gies de dÃ©ploiement (A/B testing).
- **Prometheus** : SystÃ¨me de monitoring pour la collecte des mÃ©triques techniques (notamment celles de Nginx).
- **Grafana** : Outil de visualisation pour crÃ©er des dashboards interactifs des mÃ©triques collectÃ©es.

## Objectifs du Projet

Le projet vise Ã  dÃ©montrer l'implÃ©mentation des objectifs suivants :

- **Conteneurisation** : L'API et Nginx sont empaquetÃ©s dans des conteneurs Docker distincts.
- **Orchestration** : Gestion du dÃ©marrage et de l'interaction des conteneurs via Docker Compose.
- **DÃ©ploiement avec Proxy Inverse** : Nginx route le trafic vers l'API du modÃ¨le.
- **ScalabilitÃ© & Ã‰quilibrage de Charge** : L'API est dÃ©ployÃ©e en plusieurs instances, et Nginx rÃ©partit le trafic entre elles.
- **SÃ©curitÃ© (HTTPS)** : Les communications vers Nginx sont sÃ©curisÃ©es via HTTPS avec des certificats auto-signÃ©s.
- **ContrÃ´le d'AccÃ¨s** : L'accÃ¨s Ã  l'API est protÃ©gÃ© par une authentification basique via Nginx.
- **Limitation de DÃ©bit (Rate Limiting)** : L'API est protÃ©gÃ©e contre les surcharges par Nginx.
- **A/B Testing** : Mise en place d'un routage conditionnel du trafic vers diffÃ©rentes versions de l'API basÃ© sur un en-tÃªte HTTP.
- **Monitoring** : Collecte des mÃ©triques de Nginx avec Prometheus et visualisation dans Grafana.

## Architecture ImplÃ©mentÃ©e

Le projet suit une architecture microservices oÃ¹ Nginx est le point d'entrÃ©e central gÃ©rant l'accÃ¨s aux instances de l'API et intÃ©grant le monitoring..

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”œâ”€â”€ Dockerfile     # Dockerfile de l'API v1
â”‚   â”‚   â””â”€â”€ main.py        # Code de l'API v1 FastAPI
â”‚   â”œâ”€â”€ v2/
â”‚   â”‚   â”œâ”€â”€ Dockerfile     # Dockerfile de l'API v2
â”‚   â”‚   â””â”€â”€ main.py        # Code de l'API v2 FastAPI
â”‚   â”œâ”€â”€ model.joblib       # ModÃ¨le ML prÃ©-entraÃ®nÃ©
â”‚   â””â”€â”€ requirements.txt   # DÃ©pendances Python de l'API
â”œâ”€â”€ nginx/
â”‚   â”œâ”€â”€ Dockerfile         # Dockerfile pour Nginx
â”‚   â”œâ”€â”€ nginx.conf         # Configuration Nginx
â”‚   â”œâ”€â”€ certs/             # Certificats SSL auto-signÃ©s
â”‚   â”‚   â”œâ”€â”€ nginx.crt
â”‚   â”‚   â””â”€â”€ nginx.key
â”‚   â””â”€â”€ .htpasswd          # Fichier d'authentification basique
â”œâ”€â”€ prometheus/
â”‚   â””â”€â”€ prometheus.yml     # Configuration Prometheus
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ gen_model.py       # Script pour gÃ©nÃ©rer model.joblib localement
â”‚   â””â”€â”€ tweet_emotions.csv # Dataset pour le model
â”œâ”€â”€ docker-compose.yml     # Orchestration des services
â”œâ”€â”€ Makefile               # Automatisation des tÃ¢ches
â””â”€â”€ README.md              # Ce fichier
```

## Services Docker Compose

Le fichier `docker-compose.yml` orchestre les services suivants :

- `api-v1` : Les instances de la version 1 de l'API de modÃ¨le (par dÃ©faut). `api-v1` est configurÃ© avec 3 rÃ©pliques pour l'Ã©quilibrage de charge.
- `api-v2` : L'instance de la version 2 (debug) de l'API (pour l'A/B testing).
- `nginx` : Le proxy inverse Nginx, point d'entrÃ©e principal.
- `nginx_exporter` : Collecte les mÃ©triques de Nginx pour Prometheus.
- `prometheus` : Le serveur Prometheus pour la collecte et le stockage des mÃ©triques.- `grafana` : L'interface de visualisation pour les dashboards de monitoring.

## Instructions d'Utilisation

Suivez ces Ã©tapes pour lancer et tester l'Ã©cosystÃ¨me MLOps.

1. **PrÃ©paration du Projet** : 
    - Clonez ce dÃ©pÃ´t Git.
    - **PrÃ©parez les images** : 
        Construisez les images Docker avec `make build-project`.
    
2. **Lancez tous les services** : `make run-project`.
    - Vous pouvez accÃ¨der aux interfaces :
        - Prometheus UI : `http://localhost:9090`
        - Grafana UI : `http://localhost:3000` (admin:admin)

3. **Tester le projet** :
    - Vous pouvez effectuer un test par dÃ©faut avec `make test-api`.
    - Utilisez la commande suivante pour faire des tests avec des entrÃ©es plus personalisÃ©es : 
    ```bash
    curl -X POST "https://localhost/predict" \
    -H "Content-Type: application/json" \
    -d '{"sentence": "Oh yeah, that was soooo cool!"}' \
    --user admin:admin \
    --insecure
    ```

4. **Test du Routage A/B Testing** :
    - l'entÃªte `X-Experiment-Group: debug` vous fait utiliser le service B (`api-v2`).
    - Vous pouvez effectuer un test par dÃ©faut avec `make test-api-debug`.
    - Utilisez la commande suivante pour faire des tests avec des entrÃ©es plus personalisÃ©es : 
    ```bash
    curl -X POST "https://localhost/predict" \
    -H "Content-Type: application/json" \
    -H "X-Experiment-Group: debug" \
    -d '{"sentence": "Oh yeah, that was soooo cool!"}' \
    --user admin:admin \
    --insecure
    ```
    - VÃ©rifier la rÃ©ponse, elle devrait contenir plus d'informations Ã  propos de la prÃ©diction du modÃ¨le.

5.  **ArrÃªter** :
    - ArrÃªtez les conteneurs avec `make stop-project`

## Extra

- Vous pouvez gÃ©nÃ©rer un nouveau `model.joblib` avec le fichier `src/gen_model.py`.
- On pourrait Ã©galement tester la charge et le rate limiting sur l'API principale avec la commande suivante :
```bash
for i in {1..100}; do curl -s -o /dev/null -w "%{http_code}\n" -X POST "https://localhost/predict" -H "Content-Type: application/json" -d '{"sentence": "Oh yeah, that was soooo cool!"}' --user "admin:admin" --insecure; done
```