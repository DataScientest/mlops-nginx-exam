# Examen Nginx pour MLOps ğŸš€

### Cas d'Usage et ModÃ¨le Fourni

Pour cet examen, vous travaillerez avec un modÃ¨le de reconnaissance de sentiments d'une phrase donnÃ©e.

- **ModÃ¨le** : Un modÃ¨le model.joblib prÃ©-entraÃ®nÃ©.

- **API** : Une API FastAPI simple (similaire Ã  celle du cours) qui prend une phrase en entrÃ©e et retourne le prÃ©dit.

### Fichiers Fournis** :

```
mlops-nginx-exam/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ model.joblib     # ModÃ¨le prÃ©-entraÃ®nÃ©
â”‚   â””â”€â”€ requirements.txt # DÃ©pendances Python de l'API
â”œâ”€â”€ nginx/
â”‚   â”œâ”€â”€ nginx.conf       # Votre configuration Nginx Ã  complÃ©ter
â”‚   â”œâ”€â”€ certs/           # Pour vos certificats SSL
â”‚   â””â”€â”€ .htpasswd        # Pour l'authentification basique
â”œâ”€â”€ docker-compose.yml   # Votre fichier Docker Compose Ã  complÃ©ter
â”œâ”€â”€ Makefile             # Votre Makefile Ã  complÃ©ter
â””â”€â”€ README.md
```

### Objectif du Projet

Mettre en place une architecture conteneurisÃ©e utilisant Nginx comme point d'entrÃ©e unique pour une API de modÃ¨le ML, en implÃ©mentant les fonctionnalitÃ©s suivantes :

1.  **Reverse Proxy** : Nginx doit router le trafic vers l'API du modÃ¨le.

2.  **Ã‰quilibrage de Charge** : L'API du modÃ¨le doit Ãªtre dÃ©ployÃ©e en plusieurs instances (au moins 3), et Nginx doit rÃ©partir le trafic entre elles.

3.  **HTTPS (SSL/TLS)** : Toutes les communications vers Nginx doivent Ãªtre sÃ©curisÃ©es via HTTPS, en utilisant des certificats auto-signÃ©s. Le trafic HTTP doit Ãªtre redirigÃ© vers HTTPS.

1. **PrÃ©paration du Projet** : 
    - Clonez ce dÃ©pÃ´t Git.
    
2. **Lancez tous les services** : `make start-project`.
    - Vous pouvez accÃ¨der aux interfaces :
        - Prometheus UI : `http://localhost:9090`
        - Grafana UI : `http://localhost:3000` (admin:admin)

3. **Tester le projet** :
    - Vous pouvez effectuer un test par dÃ©faut avec `make test-api`.

4. **Test du Routage A/B Testing** :
    - l'entÃªte `X-Experiment-Group: debug` vous fait utiliser le service B (`api-v2`).
    - Vous pouvez effectuer un test par dÃ©faut avec `make test-api-debug`.
    - VÃ©rifier la rÃ©ponse, elle devrait contenir plus d'informations Ã  propos de la prÃ©diction du modÃ¨le.

    - Vous devrez dÃ©ployer deux versions de l'API de modÃ¨le (par exemple, `api-v1` et `api-v2`). Ces versions peuvent Ãªtre simulÃ©es en modifiant lÃ©gÃ¨rement la rÃ©ponse de l'API (ex: ajouter un champ "version": "v1" ou "version": "v2" dans la rÃ©ponse JSON).

    - Nginx doit router le trafic vers la version v2 de l'API si la requÃªte contient un en-tÃªte HTTP spÃ©cifique (par exemple, `X-Experiment-Group: debug`).

- Vous pouvez gÃ©nÃ©rer un nouveau `model.joblib` avec le fichier `src/gen_model.py`.
- On pourrait Ã©galement tester la charge et le rate limiting sur l'API principale avec la commande suivante :
```bash
for i in {1..100}; do curl -s -o /dev/null -w "%{http_code}\n" -X POST "https://localhost/predict" -H "Content-Type: application/json" -d '{"sentence": "Oh yeah, that was soooo cool!"}' --user "admin:admin" --cacert ./deployments/nginx/certs/nginx.crt; done
```
